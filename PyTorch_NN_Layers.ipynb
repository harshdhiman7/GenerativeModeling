{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9N18E3PNNKQmSFxrYVYhv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshdhiman7/GenerativeModeling/blob/main/PyTorch_NN_Layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LinearLayer (Fully Connected Layer):"
      ],
      "metadata": {
        "id": "Kzuf5Ew4v5gY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm29p-p4v0qN",
        "outputId": "36a29dcb-5836-4ed2-f17c-e75772365fee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input layer has a shape :  torch.Size([64, 100])\n",
            "Output layer has a shape :  torch.Size([64, 50])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define input and output sizes\n",
        "input_size = 100\n",
        "output_size = 50\n",
        "\n",
        "# Create a linear layer\n",
        "linear_layer = nn.Linear(input_size, output_size)\n",
        "\n",
        "# Example usage:\n",
        "input_data = torch.randn(64, input_size)  # Batch size of 64\n",
        "print(\"Input layer has a shape : \",input_data.shape)\n",
        "output_data = linear_layer(input_data)\n",
        "print(\"Output layer has a shape : \",output_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Convolutional Layer:"
      ],
      "metadata": {
        "id": "jXCfgp9AwFQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define input channels, output channels, and kernel size\n",
        "input_channels = 3\n",
        "output_channels = 64\n",
        "kernel_size = 3\n",
        "\n",
        "# Create a convolutional layer\n",
        "conv_layer = nn.Conv2d(input_channels, output_channels, kernel_size)\n",
        "\n",
        "# Example usage:\n",
        "input_data = torch.randn(32, input_channels, 128, 128)  # Batch size of 32, 128x128 image\n",
        "print(input_data.shape)\n",
        "output_data = conv_layer(input_data)\n",
        "print(\"Output layer has a shape : \",output_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiUmMawuwIKq",
        "outputId": "f98de444-37f3-4a64-b13a-45c30305b4bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 128, 128])\n",
            "Output layer has a shape :  torch.Size([32, 64, 126, 126])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation Function (e.g., ReLU):"
      ],
      "metadata": {
        "id": "g9Ih1DG4wZJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_data = torch.randn(64, 100)\n",
        "input_dim= input_data.shape[1]\n",
        "\n",
        "# Create a ReLU activation layer\n",
        "relu_layer = nn.ReLU()\n",
        "\n",
        "# Example usage:\n",
        "linear_layer_1=nn.Linear(input_dim,50)\n",
        "linear_layer_1=linear_layer_1(input_data)\n",
        "\n",
        "output_data = relu_layer(linear_layer_1)\n",
        "\n",
        "print(\"Output layer has a shape : \",output_data.shape)\n",
        "print(\"The output after Linear+ReLU operation is: \",output_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQXjOhJDwcJb",
        "outputId": "9f7884d7-212f-4cd1-a716-a6da3631d24e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output layer has a shape :  torch.Size([64, 50])\n",
            "The output after Linear+ReLU operation is:  tensor([[0.0000, 0.4546, 0.0000,  ..., 0.2794, 0.0000, 0.0000],\n",
            "        [0.1992, 0.0000, 0.0000,  ..., 0.0000, 0.4400, 0.0000],\n",
            "        [0.0820, 0.0000, 0.1256,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.2200, 0.2119,  ..., 0.3813, 0.2281, 1.1401],\n",
            "        [0.8858, 0.0000, 0.0000,  ..., 0.0000, 0.3795, 0.3558],\n",
            "        [0.0000, 0.5104, 0.0000,  ..., 0.5599, 0.2337, 0.6868]],\n",
            "       grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Normalization"
      ],
      "metadata": {
        "id": "7QdeAv3AwgFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the number of features\n",
        "num_features = 50\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "input_data = torch.randn(32, num_features)  # Batch size of 32\n",
        "\n",
        "linear_layer_1=nn.Linear(num_features,10)\n",
        "linear_layer_1=linear_layer_1(input_data)\n",
        "\n",
        "print(\"Before Batch Normalization :\",linear_layer_1[0])\n",
        "\n",
        "batch_norm_layer = nn.BatchNorm1d(linear_layer_1.shape[1])\n",
        "batch_norm_layer = batch_norm_layer(linear_layer_1)\n",
        "\n",
        "print(\"After Batch Normalization is: \",batch_norm_layer[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0zVtOnpwkye",
        "outputId": "ad55c1da-1357-4d12-8c48-f88c82c9fb33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Batch Normalization : tensor([-0.9090, -0.8179, -0.7119, -0.5541,  0.5397,  0.2693,  1.0204,  1.0115,\n",
            "        -0.0946,  0.2200], grad_fn=<SelectBackward0>)\n",
            "After Batch Normalization is:  tensor([-1.2250, -1.3786, -1.1051, -0.7907,  0.3991,  1.0098,  1.3480,  1.4936,\n",
            "        -0.3140,  0.4658], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropout"
      ],
      "metadata": {
        "id": "tPIymkp5wq43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the dropout probability\n",
        "dropout_prob = 0.5\n",
        "\n",
        "# Create a dropout layer\n",
        "dropout_layer = nn.Dropout(dropout_prob)\n",
        "\n",
        "# Example usage:\n",
        "input_data = torch.randn(64, 100)\n",
        "output_data = dropout_layer(input_data)\n",
        "print(\"Output layer has a shape : \",output_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehzqVAO8wspH",
        "outputId": "da6b55cb-bed5-4d37-ca8a-4c282ede6bce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output layer has a shape :  torch.Size([64, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN layer"
      ],
      "metadata": {
        "id": "LyCIO2_Sw3Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define input size, hidden size, and number of recurrent layers\n",
        "input_size = 10\n",
        "hidden_size = 20\n",
        "num_layers = 2\n",
        "\n",
        "# Create an RNN layer\n",
        "rnn_layer = nn.RNN(input_size, hidden_size, num_layers)\n",
        "\n",
        "# Example usage:\n",
        "input_data = torch.randn(32, 5, input_size)  # Sequence length of 5, batch size of 32\n",
        "print(\"The shape of the input data is: \", input_data.shape)\n",
        "output_data, hidden_state = rnn_layer(input_data)\n",
        "print(\"Output layer has a shape : \",output_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fnpjKeaw404",
        "outputId": "0aed69ad-6e99-410e-f3ab-c543716ec7f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the input data is:  torch.Size([32, 5, 10])\n",
            "Output layer has a shape :  torch.Size([32, 5, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Layer"
      ],
      "metadata": {
        "id": "FSrFuzYqw80o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define input size, hidden size, and number of recurrent layers\n",
        "input_size = 10\n",
        "hidden_size = 20\n",
        "num_layers = 2\n",
        "\n",
        "# Create an LSTM layer\n",
        "lstm_layer = nn.LSTM(input_size, hidden_size, num_layers)\n",
        "\n",
        "# Example usage:\n",
        "input_data = torch.randn(32, 5, input_size)  # Sequence length of 5, batch size of 32\n",
        "output_data, (hidden_state, cell_state) = lstm_layer(input_data)\n",
        "print(\"Output layer has a shape : \",output_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiLSqNLtw_zZ",
        "outputId": "a4490896-c528-44d5-988b-b943fd1c20c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output layer has a shape :  torch.Size([32, 5, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU Layer"
      ],
      "metadata": {
        "id": "K-Gvode1xFvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define input size, hidden size, and number of recurrent layers\n",
        "input_size = 10\n",
        "hidden_size = 20\n",
        "num_layers = 2\n",
        "\n",
        "# Create a GRU layer\n",
        "gru_layer = nn.GRU(input_size, hidden_size, num_layers)\n",
        "\n",
        "# Example usage:\n",
        "input_data = torch.randn(32, 5, input_size)  # Sequence length of 5, batch size of 32\n",
        "output_data, hidden_state = gru_layer(input_data)\n",
        "print(\"Output layer has a shape : \",output_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ls1Qfo6uxIuI",
        "outputId": "a3a2dbe4-13c4-41f5-c9bd-ed7dde00d550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output layer has a shape :  torch.Size([32, 5, 20])\n"
          ]
        }
      ]
    }
  ]
}