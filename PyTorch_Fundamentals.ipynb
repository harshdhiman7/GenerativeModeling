{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshdhiman7/GenerativeModeling/blob/main/PyTorch_Fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PyTorch Fundamentals: Tensors, Indexing, Linear Algebra, Norms"
      ],
      "metadata": {
        "id": "lkHuk2AD6keF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfumDUHZUtI4",
        "outputId": "92454865-2255-4b43-c6f3-f68fa63d3a85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor from list: tensor([1., 2., 3., 4.])\n",
            "Tensor shape is  torch.Size([4])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "list_1=[1,2,3,4]\n",
        "# Create a tensor from a Python list\n",
        "tensor_from_list = torch.Tensor(list_1)\n",
        "print(\"Tensor from list:\", tensor_from_list)\n",
        "print(\"Tensor shape is \",tensor_from_list.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor of zeros with a specified shape\n",
        "zeros_tensor = torch.Tensor(2, 3).zero_()\n",
        "print(\"Zeros tensor:\", zeros_tensor)\n",
        "\n",
        "# Create a tensor of ones with a specified shape\n",
        "ones_tensor = torch.Tensor(3, 2).fill_(1)\n",
        "print(\"Ones tensor:\", ones_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlO5YXklPlaV",
        "outputId": "811beb49-7906-4da9-f1a1-caa0ca8350a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zeros tensor: tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "Ones tensor: tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create two tensors for demonstration\n",
        "tensor_a = torch.Tensor([1, 2, 3])\n",
        "tensor_b = torch.Tensor([4, 5, 6])\n",
        "\n",
        "# Addition\n",
        "result_addition = tensor_a + tensor_b\n",
        "print(\"Element-wise addition:\", result_addition)\n",
        "\n",
        "# Subtraction\n",
        "result_subtraction = tensor_a - tensor_b\n",
        "print(\"Element-wise subtraction:\", result_subtraction)\n",
        "\n",
        "# Multiplication\n",
        "result_multiplication = tensor_a * tensor_b\n",
        "print(\"Element-wise multiplication:\", result_multiplication)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX9QvCVrjEHr",
        "outputId": "56dde546-5499-4b19-e09d-4456a58daf35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Element-wise addition: tensor([5., 7., 9.])\n",
            "Element-wise subtraction: tensor([-3., -3., -3.])\n",
            "Element-wise multiplication: tensor([ 4., 10., 18.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Create a tensor for demonstration\n",
        "tensor_example = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "# Accessing a specific element\n",
        "element_at_index_1_1 = tensor_example[1, 1]\n",
        "print(\"Element at index (1, 1):\", element_at_index_1_1)\n",
        "\n",
        "# Slicing along rows and columns\n",
        "row_slice = tensor_example[0:2, :]\n",
        "column_slice = tensor_example[:, 1]\n",
        "print(\"Sliced rows:\", row_slice)\n",
        "print(\"Sliced column:\", column_slice)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDkNH1TMjg54",
        "outputId": "f784bc1e-a204-46f2-9467-fee40a646140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Element at index (1, 1): tensor(5.)\n",
            "Sliced rows: tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "Sliced column: tensor([2., 5., 8.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Matrix Operations"
      ],
      "metadata": {
        "id": "_1xMUJYM5hZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix multiplication\n",
        "matrix_a = torch.tensor([[1, 2], [3, 4]])\n",
        "matrix_b = torch.tensor([[5, 6], [7, 8]])\n",
        "result = torch.mm(matrix_a, matrix_b)"
      ],
      "metadata": {
        "id": "jlI9icyE5mN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matrix Inversion"
      ],
      "metadata": {
        "id": "uzkC-LXe5scU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix inversion\n",
        "inverse_matrix = torch.inverse(matrix)\n",
        "print(inverse_matrix)"
      ],
      "metadata": {
        "id": "V8x6sEUh5wL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EigenValues & EigenVectors"
      ],
      "metadata": {
        "id": "DCXJnaz353Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eigenvalues and eigenvectors\n",
        "eigenvalues, eigenvectors = torch.eig(matrix, eigenvectors=True)"
      ],
      "metadata": {
        "id": "T5egl4ej59l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Norms"
      ],
      "metadata": {
        "id": "U77Phtg-5_a2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# L2 norm\n",
        "l2_norm = torch.norm(vector)\n",
        "\n",
        "# Frobenius norm (for matrices)\n",
        "frobenius_norm = torch.norm(matrix)"
      ],
      "metadata": {
        "id": "_j1fsB_L6JVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Solve System of Linear Equations"
      ],
      "metadata": {
        "id": "WHu4VD5W8Txs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the coefficient matrix A and the right-hand side vector b\n",
        "A = torch.tensor([[2.0, -1.0, 3.0],\n",
        "                  [1.0, 1.0, 1.0],\n",
        "                  [3.0, 2.0, -2.0]])\n",
        "\n",
        "b = torch.tensor([8.0, 4.0, 10.0])\n",
        "\n",
        "# Solve the system of linear equations Ax = b\n",
        "x, _ = torch.solve(b.unsqueeze(1), A)\n",
        "x = x.squeeze()\n",
        "\n",
        "print(\"Solution x:\", x)\n"
      ],
      "metadata": {
        "id": "t-xrSxQh8XKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gradient Computation\n",
        "import torch\n",
        "\n",
        "# Create a tensor for which we want to compute gradients\n",
        "tensor_for_gradients = torch.tensor([2.0], requires_grad=True)\n",
        "\n",
        "# Perform a computation involving the tensor\n",
        "result = tensor_for_gradients ** 2\n",
        "\n",
        "# Backward pass to compute gradients\n",
        "result.backward()\n",
        "\n",
        "# Access the computed gradient\n",
        "gradient = tensor_for_gradients.grad\n",
        "print(\"Gradient of the tensor:\", gradient)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oeU0F14k2zG",
        "outputId": "e7f3a9ee-506e-49f5-f438-baef048d4832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of the tensor: tensor([4.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement a simple feedforward neural network using PyTorch.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple feedforward neural network class\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size_1,hidden_size_2, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size_1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size_1,hidden_size_2)\n",
        "        self.relu= nn.ReLU()\n",
        "        self.fc3=  nn.Linear(hidden_size_2,output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Create an instance of the SimpleNN class\n",
        "input_size = 3\n",
        "hidden_size_1 = 5\n",
        "hidden_size_2= 3\n",
        "output_size = 1\n",
        "\n",
        "model = SimpleNN(input_size, hidden_size_1,hidden_size_2,output_size)\n",
        "\n",
        "# Define a binary cross-entropy loss function and an optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Dummy input data (replace this with your actual data)\n",
        "input_data = torch.randn(10, input_size)\n",
        "\n",
        "# Target labels (binary classification task)\n",
        "target_labels = torch.randint(0, 2, (10, 1)).float()\n",
        "\n",
        "# Training loop\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(input_data)\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = criterion(outputs, target_labels)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss every 100 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate the trained model\n",
        "with torch.no_grad():\n",
        "    # Test the model on new data\n",
        "    test_input = torch.randn(5, input_size)\n",
        "    predictions = model(test_input)\n",
        "    predicted_labels = (predictions > 0.5).float()\n",
        "\n",
        "    print(\"\\nPredictions:\")\n",
        "    print(predicted_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy8zQbjQnY8m",
        "outputId": "4fdfe659-fb6e-42d5-91c1-39cfc68e5a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 0.5052\n",
            "Epoch [200/1000], Loss: 0.3395\n",
            "Epoch [300/1000], Loss: 0.2231\n",
            "Epoch [400/1000], Loss: 0.1497\n",
            "Epoch [500/1000], Loss: 0.1048\n",
            "Epoch [600/1000], Loss: 0.0766\n",
            "Epoch [700/1000], Loss: 0.0583\n",
            "Epoch [800/1000], Loss: 0.0460\n",
            "Epoch [900/1000], Loss: 0.0373\n",
            "Epoch [1000/1000], Loss: 0.0309\n",
            "\n",
            "Predictions:\n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, output_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = self.tanh(x)\n",
        "        return x\n",
        "\n",
        "# Define Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Hyperparameters\n",
        "latent_dim = 100\n",
        "generator_input_size = 100\n",
        "generator_output_size = 784  # Example for MNIST-like data\n",
        "discriminator_input_size = 784  # Example for MNIST-like data\n",
        "learning_rate = 0.0002\n",
        "epochs = 10000\n",
        "batch_size = 64\n",
        "\n",
        "# Initialize Generator and Discriminator\n",
        "generator = Generator(generator_input_size, generator_output_size)\n",
        "discriminator = Discriminator(discriminator_input_size)\n",
        "\n",
        "# Define loss function and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_g = optim.Adam(generator.parameters(), lr=learning_rate)\n",
        "optimizer_d = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(epochs):\n",
        "    # Sample random noise for the generator\n",
        "    noise = torch.randn(batch_size, latent_dim)\n",
        "\n",
        "    # Generate fake data using the generator\n",
        "    fake_data = generator(noise)\n",
        "\n",
        "    # Discriminator forward pass for real data\n",
        "    real_labels = torch.ones(batch_size, 1)\n",
        "    outputs_real = discriminator(fake_data.detach())\n",
        "\n",
        "    # Discriminator forward pass for fake data\n",
        "    fake_labels = torch.zeros(batch_size, 1)\n",
        "    outputs_fake = discriminator(fake_data)\n",
        "\n",
        "    # Discriminator loss and backpropagation\n",
        "    loss_real = criterion(outputs_real, real_labels)\n",
        "    loss_fake = criterion(outputs_fake, fake_labels)\n",
        "    loss_discriminator = loss_real + loss_fake\n",
        "    optimizer_d.zero_grad()\n",
        "    loss_discriminator.backward()\n",
        "    optimizer_d.step()\n",
        "\n",
        "    # Generate new random noise for the generator\n",
        "    noise = torch.randn(batch_size, latent_dim)\n",
        "\n",
        "    # Generate fake data using the updated generator\n",
        "    fake_data = generator(noise)\n",
        "\n",
        "    # Discriminator forward pass for generated fake data\n",
        "    outputs_fake = discriminator(fake_data)\n",
        "\n",
        "    # Generator loss and backpropagation\n",
        "    loss_generator = criterion(outputs_fake, real_labels)\n",
        "    optimizer_g.zero_grad()\n",
        "    loss_generator.backward()\n",
        "    optimizer_g.step()\n",
        "\n",
        "    # Print the loss every 100 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss D: {loss_discriminator.item():.4f}, Loss G: {loss_generator.item():.4f}')\n",
        "\n",
        "# After training, you can generate samples using the trained generator\n",
        "with torch.no_grad():\n",
        "    # Generate new random noise for sampling\n",
        "    sample_noise = torch.randn(16, latent_dim)\n",
        "\n",
        "    # Generate samples using the trained generator\n",
        "    generated_samples = generator(sample_noise)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iAIH3r0MgQ0",
        "outputId": "4cf6a810-0000-47a4-d68e-0d3d8d1d68a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/10000], Loss D: 1.3888, Loss G: 0.6679\n",
            "Epoch [200/10000], Loss D: 1.3875, Loss G: 0.6784\n",
            "Epoch [300/10000], Loss D: 1.3876, Loss G: 0.6801\n",
            "Epoch [400/10000], Loss D: 1.3869, Loss G: 0.6887\n",
            "Epoch [500/10000], Loss D: 1.3871, Loss G: 0.6874\n",
            "Epoch [600/10000], Loss D: 1.3869, Loss G: 0.6880\n",
            "Epoch [700/10000], Loss D: 1.3868, Loss G: 0.6889\n",
            "Epoch [800/10000], Loss D: 1.3866, Loss G: 0.6850\n",
            "Epoch [900/10000], Loss D: 1.3865, Loss G: 0.6906\n",
            "Epoch [1000/10000], Loss D: 1.3864, Loss G: 0.6903\n",
            "Epoch [1100/10000], Loss D: 1.3865, Loss G: 0.6917\n",
            "Epoch [1200/10000], Loss D: 1.3864, Loss G: 0.6926\n",
            "Epoch [1300/10000], Loss D: 1.3864, Loss G: 0.6911\n",
            "Epoch [1400/10000], Loss D: 1.3863, Loss G: 0.6915\n",
            "Epoch [1500/10000], Loss D: 1.3864, Loss G: 0.6923\n",
            "Epoch [1600/10000], Loss D: 1.3863, Loss G: 0.6932\n",
            "Epoch [1700/10000], Loss D: 1.3863, Loss G: 0.6918\n",
            "Epoch [1800/10000], Loss D: 1.3863, Loss G: 0.6918\n",
            "Epoch [1900/10000], Loss D: 1.3863, Loss G: 0.6916\n",
            "Epoch [2000/10000], Loss D: 1.3863, Loss G: 0.6932\n",
            "Epoch [2100/10000], Loss D: 1.3863, Loss G: 0.6928\n",
            "Epoch [2200/10000], Loss D: 1.3863, Loss G: 0.6936\n",
            "Epoch [2300/10000], Loss D: 1.3863, Loss G: 0.6924\n",
            "Epoch [2400/10000], Loss D: 1.3863, Loss G: 0.6924\n",
            "Epoch [2500/10000], Loss D: 1.3863, Loss G: 0.6927\n",
            "Epoch [2600/10000], Loss D: 1.3863, Loss G: 0.6939\n",
            "Epoch [2700/10000], Loss D: 1.3863, Loss G: 0.6931\n",
            "Epoch [2800/10000], Loss D: 1.3863, Loss G: 0.6924\n",
            "Epoch [2900/10000], Loss D: 1.3863, Loss G: 0.6932\n",
            "Epoch [3000/10000], Loss D: 1.3863, Loss G: 0.6935\n",
            "Epoch [3100/10000], Loss D: 1.3863, Loss G: 0.6930\n",
            "Epoch [3200/10000], Loss D: 1.3863, Loss G: 0.6928\n",
            "Epoch [3300/10000], Loss D: 1.3863, Loss G: 0.6933\n",
            "Epoch [3400/10000], Loss D: 1.3863, Loss G: 0.6932\n",
            "Epoch [3500/10000], Loss D: 1.3863, Loss G: 0.6934\n",
            "Epoch [3600/10000], Loss D: 1.3863, Loss G: 0.6932\n",
            "Epoch [3700/10000], Loss D: 1.3863, Loss G: 0.6934\n",
            "Epoch [3800/10000], Loss D: 1.3863, Loss G: 0.6931\n",
            "Epoch [3900/10000], Loss D: 1.3863, Loss G: 0.6933\n",
            "Epoch [4000/10000], Loss D: 1.3863, Loss G: 0.6927\n",
            "Epoch [4100/10000], Loss D: 1.3863, Loss G: 0.6935\n",
            "Epoch [4200/10000], Loss D: 1.3863, Loss G: 0.6935\n",
            "Epoch [4300/10000], Loss D: 1.3863, Loss G: 0.6933\n",
            "Epoch [4400/10000], Loss D: 1.3863, Loss G: 0.6932\n",
            "Epoch [4500/10000], Loss D: 1.3863, Loss G: 0.6931\n",
            "Epoch [4600/10000], Loss D: 1.3863, Loss G: 0.6935\n",
            "Epoch [4700/10000], Loss D: 1.3863, Loss G: 0.6930\n",
            "Epoch [4800/10000], Loss D: 1.3863, Loss G: 0.6930\n",
            "Epoch [4900/10000], Loss D: 1.3863, Loss G: 0.6930\n",
            "Epoch [5000/10000], Loss D: 1.3863, Loss G: 0.6930\n",
            "Epoch [5100/10000], Loss D: 1.3863, Loss G: 0.6929\n",
            "Epoch [5200/10000], Loss D: 1.3863, Loss G: 0.6928\n",
            "Epoch [5300/10000], Loss D: 1.3863, Loss G: 0.6931\n",
            "Epoch [5400/10000], Loss D: 1.3863, Loss G: 0.6936\n",
            "Epoch [5500/10000], Loss D: 1.3863, Loss G: 0.6930\n",
            "Epoch [5600/10000], Loss D: 1.3863, Loss G: 0.6932\n",
            "Epoch [5700/10000], Loss D: 1.3863, Loss G: 0.6929\n",
            "Epoch [5800/10000], Loss D: 1.3863, Loss G: 0.6929\n",
            "Epoch [5900/10000], Loss D: 1.3863, Loss G: 0.6932\n",
            "Epoch [6000/10000], Loss D: 1.3863, Loss G: 0.6932\n",
            "Epoch [6100/10000], Loss D: 1.3863, Loss G: 0.6928\n",
            "Epoch [6200/10000], Loss D: 1.3863, Loss G: 0.6933\n",
            "Epoch [6300/10000], Loss D: 1.3863, Loss G: 0.6934\n",
            "Epoch [6400/10000], Loss D: 1.3863, Loss G: 0.6939\n",
            "Epoch [6500/10000], Loss D: 1.3863, Loss G: 0.6933\n",
            "Epoch [6600/10000], Loss D: 1.3863, Loss G: 0.6933\n",
            "Epoch [6700/10000], Loss D: 1.3863, Loss G: 0.6919\n",
            "Epoch [6800/10000], Loss D: 1.3863, Loss G: 0.6923\n",
            "Epoch [6900/10000], Loss D: 1.3863, Loss G: 0.6937\n",
            "Epoch [7000/10000], Loss D: 1.3863, Loss G: 0.6930\n",
            "Epoch [7100/10000], Loss D: 1.3863, Loss G: 0.6931\n",
            "Epoch [7200/10000], Loss D: 1.3863, Loss G: 0.6929\n",
            "Epoch [7300/10000], Loss D: 1.3863, Loss G: 0.6931\n",
            "Epoch [7400/10000], Loss D: 1.3863, Loss G: 0.6938\n",
            "Epoch [7500/10000], Loss D: 1.3863, Loss G: 0.6935\n",
            "Epoch [7600/10000], Loss D: 1.3863, Loss G: 0.6933\n",
            "Epoch [7700/10000], Loss D: 1.3863, Loss G: 0.6921\n",
            "Epoch [7800/10000], Loss D: 1.3863, Loss G: 0.6931\n",
            "Epoch [7900/10000], Loss D: 1.3863, Loss G: 0.6936\n",
            "Epoch [8000/10000], Loss D: 1.3863, Loss G: 0.6927\n",
            "Epoch [8100/10000], Loss D: 1.3863, Loss G: 0.6935\n",
            "Epoch [8200/10000], Loss D: 1.3863, Loss G: 0.6932\n",
            "Epoch [8300/10000], Loss D: 1.3863, Loss G: 0.6939\n",
            "Epoch [8400/10000], Loss D: 1.3863, Loss G: 0.6943\n",
            "Epoch [8500/10000], Loss D: 1.3863, Loss G: 0.6949\n",
            "Epoch [8600/10000], Loss D: 1.3863, Loss G: 0.6937\n",
            "Epoch [8700/10000], Loss D: 1.3863, Loss G: 0.6929\n",
            "Epoch [8800/10000], Loss D: 1.3863, Loss G: 0.6935\n",
            "Epoch [8900/10000], Loss D: 1.3863, Loss G: 0.6929\n",
            "Epoch [9000/10000], Loss D: 1.3863, Loss G: 0.6925\n",
            "Epoch [9100/10000], Loss D: 1.3863, Loss G: 0.6938\n",
            "Epoch [9200/10000], Loss D: 1.3863, Loss G: 0.6941\n",
            "Epoch [9300/10000], Loss D: 1.3863, Loss G: 0.6925\n",
            "Epoch [9400/10000], Loss D: 1.3863, Loss G: 0.6931\n",
            "Epoch [9500/10000], Loss D: 1.3863, Loss G: 0.6922\n",
            "Epoch [9600/10000], Loss D: 1.3863, Loss G: 0.6932\n",
            "Epoch [9700/10000], Loss D: 1.3863, Loss G: 0.6922\n",
            "Epoch [9800/10000], Loss D: 1.3863, Loss G: 0.6932\n",
            "Epoch [9900/10000], Loss D: 1.3863, Loss G: 0.6931\n",
            "Epoch [10000/10000], Loss D: 1.3863, Loss G: 0.6924\n"
          ]
        }
      ]
    }
  ]
}